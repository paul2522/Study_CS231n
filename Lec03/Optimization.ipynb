{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 거대한 골짜기를 내려간다고 할때 가장 낮은데를 찾는건 쉽지 않다.\n",
    "* 산들의 높이를 W로 인해 발생한 loss 라고 본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Random Search : W 묶음을 랜덤하게 설정 - 안좋은 알고리즘\n",
    "* Linear Classifier를 Random Search로 훈련시키면 -> 15%\n",
    "* 그냥 무작위로 고르는 것보단 좋지만, 최첨단 기법을 쓰면 95%를 뽑는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 산 내려가는 것을 예시로 들면 정확한 저지대의 위치를 몰라도 땅을 밟을 때 느껴지는 경사도를 통해 방향을 알 수 있다.\n",
    "* 여기서 slope는 함수의 미분이라고 볼 수 있다.\n",
    "* 현재 위치와 진짜 작은 한걸음 이동 후 위치의 차이."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* df(x)/dx = lim h->0 f(x+h) - f(x)/h\n",
    "* 위 식에서 우리는 x가 스칼라 값이 아닌 벡터이므로 이 식을 일반화하여 다변수의 개념으로 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 다변수의 미분을 통해 일반화를 한 것이 gradient\n",
    "* gradient는 벡터의 편미분입니다.\n",
    "* gradient는 x와 같은 형태이며, gradient의 각 요소는 함수 f에서 해당 좌표 방향으로 움직일 때의 경사를 말합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* gradient의 특징\n",
    "  - 양의 gradient 방향은 함수의 최대 증가 방향을 가리킴\n",
    "  - 마찬가지로 음의 gradient 방향은 함수의 최대 감소 방향을 가리킴\n",
    "  - 전체에 대한 기울기는 알려면 gradient의 unit vector들의 dot 결과를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* gradient는 함수의 해당 위치의 일차 선형의 근사를 가르쳐줌\n",
    "* Deep learning은 수많은 함수의 gradient 계산 그리고 그 gradient는 순차적으로 파라미터 벡터를 변화시킴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 단순하게 컴퓨터의 gradient를 평가하려면 유한차분법을 사용(유한차분법 h 이동 한거의 값과 그 전값을 빼고 h로 나눔)\n",
    "* 기울기의 한계 정의로 돌아가기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* current W -> ? -> gradient dW\n",
    "* W, dW 같은 shape\n",
    "* W의 x1의 loss 값 = W_loss과  x1+h(0.0001)의 loss 값 = W+h_loss\n",
    "* gradient x1 = ((W_loss) - (W+h_loss))/h\n",
    "* 이 짓은 x1 ~ xn 반복\n",
    "* 안좋은 짓 - 너무 느려(n이 엄청 크다면 계산 할 것이 엄청 많아진다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미적분을 통해 하나의 표현식으로 구할 수 있다.  \n",
    "그 표현식과 W를 곱해서 바로 구함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numeric Gradient는 잘 안쓰지만 Analytic Gradient가 제대로 사용되는지 디버깅하려고 쓴다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Gradient Descent\n",
    "1. W 랜덤으로 잡음\n",
    "2. loss, gradient 구함\n",
    "3. W를 gradient 방향의 반대로 update\n",
    "4. 결국 종착지에 감"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient를 구하고 그 방향으로 얼마나 움직일 것인지는 hyper-parameter  \n",
    "step_size = learning_rate  \n",
    "주로 첫번째로 설정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss : 얼마나 나쁜지  \n",
    "gradient loss 구하는데 N(data 개수)값이 너무 크면 너무 오래 걸린다.  \n",
    "그래서 사용하는게 stochastic gradient descent(확률적 경사 하강법)  \n",
    "전체 train data 대신 작은 부분인 mini batch만 사용해서 전체를 예측  \n",
    "한번 돌리는게 1 epoch\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
